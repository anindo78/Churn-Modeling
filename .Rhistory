q()
install.Packages("twitteR")
install.Packages("twitteR")
install.packages("twitteR")
library(twitteR)
requestURL <-  "https://api.twitter.com/oauth/request_token"
accessURL <-  "https://api.twitter.com/oauth/access_token"
authURL      <-  "https://api.twitter.com/oauth/authorize"
consumerKey <-  "dHY0E7a33pUgRjQ1GzJdiEGrH" # See my blog Part1
consumerSecret <-  "okoJ6xfm08pTjswqwm2ZqiYmdIhQrnRQB1b75XfVcwiB4gU3Q7" # See my blog Part1
Cred <- OAuthFactory$new(consumerKey=consumerKey,
consumerSecret=consumerSecret,
requestURL=requestURL,
accessURL=accessURL,
authURL=authURL)
library(twitteR)
Cred <- OAuthFactory$new(consumerKey=consumerKey,
consumerSecret=consumerSecret,
requestURL=requestURL,
accessURL=accessURL,
authURL=authURL)
?OAuthFactory
??OAuthFactory
install.packages("ROAuth")
library(ROAuth)
Cred <- OAuthFactory$new(consumerKey=consumerKey,
consumerSecret=consumerSecret,
requestURL=requestURL,
accessURL=accessURL,
authURL=authURL)
download.file(url="http://curl.haxx.se/ca/cacert.pem", destfile="cacert.pem")
Cred$handshake(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl"))
Cred$handshake(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl"))
Cred$handshake(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl"))
registerTwitterOAuth(Cred)
registerTwitterOAuth(Cred)
?setup_twitter_oauth
registerTwitterOAuth(Cred)
setup_twitter_oauth(Cred)
setup_twitter_oauth(Cred)
setup_twitter_oauth(Cred)
accesstoken <- "816106213-B5fOAMF7cmKWkhYhT4YurrSQmO8cuzV0mRUB4ujq"
access_secret <- "RKmvFvOZXdNOZX3K1Cr3WB2b9XSDYagTImqM0o6Tqe0dJ"
setup_twitter_oauth(consumerKey,consumerSecret, accesstoken, access_secret)
setup_twitter_oauth(consumerKey,consumerSecret, accesstoken, access_secret)
tweets <- searchTwitter("Servion", n=100, lang="en", since="2015-07-01")
?searchTwitter
??base44enc
library(RCurl)
tweets <- searchTwitter("Servion", n=100, lang="en", since="2015-07-01")
install.packages('base64enc')
tweets <- searchTwitter("Servion", n=100, lang="en", since="2015-07-01")
tweets.df <- twListToDF(tweets)
View(tweets.df)
tweets <- searchTwitter("Servion", n=100, lang="en", since="2015-01-01")
SrvTweets <- userTimeline("Servion", n=10)
SrvTweets
install.packages("tm")
library(tm)
myCorpus <- Corpus(VectorSource(df$text))
myCorpus <- Corpus(VectorSource(tweets$text))
myCorpus <- tm_map(myCorpus, tolower)
myCorpus <- tm_map(myCorpus, removePunctuation)
myCorpus <- tm_map(myCorpus, removeNumbers)
myStopwords <- c(stopwords('english'), "available", "via")
idx <- which(myStopwords == "r")
myStopwords <- myStopwords[-idx]
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
myStopwords <- c(stopwords('english'), "available", "via")
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
inspect(myCorpus[1:3])
myCorpus <- tm_map(myCorpus, stemDocument)
inspect(myCorpus[1:3])
myCorpus <- Corpus(VectorSource(tweets$text))
myCorpus <- tm_map(myCorpus, tolower)
myCorpus <- tm_map(myCorpus, removePunctuation)
myCorpus <- tm_map(myCorpus, removeNumbers)
View(tweets.df)
myCorpus <- Corpus(VectorSource(tweets$text))
View(tweets.df)
myCorpus
?VectorSource
myCorpus <- Corpus(VectorSource(tweets.df$text))
myCorpus <- tm_map(myCorpus, tolower)
myCorpus <- tm_map(myCorpus, removePunctuation)
myCorpus <- tm_map(myCorpus, removeNumbers)
myStopwords <- c(stopwords('english'), "available", "via")
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
inspect(myCorpus[1:3])
myDtm <- TermDocumentMatrix(myCorpus, control = list(minWordLength = 1))
??TermDocumentMatrix
myCorpus <- tm_map(myCorpus, stemDocument)
install.packages("Snowball")
install.packages("SnowballC")
library(SnowballC)
myCorpus <- tm_map(myCorpus, stemDocument)
inspect(myCorpus[1:3])
myCorpus <- tm_map(myCorpus, stemCompletion, dictionary=dictCorpus)
myCorpus <- Corpus(VectorSource(tweets.df$text))
myCorpus <- tm_map(myCorpus, tolower)
myCorpus <- tm_map(myCorpus, removePunctuation)
myCorpus <- tm_map(myCorpus, removeNumbers)
myStopwords <- c(stopwords('english'), "available", "via")
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
dictCorpus <- myCorpus
myCorpus <- tm_map(myCorpus, stemDocument)
myCorpus <- tm_map(myCorpus, stemDocument)
myCorpus <- tm_map(myCorpus, stemCompletion, dictionary=dictCorpus)
myCorpus <- tm_map(myCorpus, stemCompletion, dictionary=dictCorpus)
myDtm <- TermDocumentMatrix(myCorpus, control = list(minWordLength = 1))
myCorpus <- tm_map(myCorpus, stemCompletion, dictionary=dictCorpus)
myCorpus <- tm_map(content_transformer(myCorpus), stemCompletion, dictionary=dictCorpus)
myCorpus <- tm_map((myCorpus, content_transformer(stemCompletion), dictionary=dictCorpus)
myCorpus <- tm_map(myCorpus, content_transformer(stemCompletion), dictionary=dictCorpus)
myCorpus <- tm_map(myCorpus, stemCompletion), dictionary=content_transformer(dictCorpus))
myCorpus <- tm_map(myCorpus, stemCompletion, dictionary=content_transformer(dictCorpus))
myCorpus <- tm_map(content_transformer(myCorpus), stemCompletion, dictionary=content_transformer(dictCorpus))
myCorpus <- Corpus(VectorSource(tweets.df$text))
myCorpus <- tm_map(myCorpus, tolower)
myCorpus <- tm_map(myCorpus, PlainTextDocument)
myCorpus <- tm_map(myCorpus, removePunctuation)
myCorpus <- tm_map(myCorpus, removeNumbers)
myStopwords <- c(stopwords('english'), "available", "via")
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
dictCorpus <- myCorpus
myCorpus <- tm_map(myCorpus, stemDocument)
myCorpus <- tm_map(content_transformer(myCorpus), stemCompletion, dictionary=content_transformer(dictCorpus))
myCorpus <- tm_map(myCorpus, stemCompletion, dictionary=dictCorpus)
warnings()
inspect(myCorpus[1:3])
myDtm <- TermDocumentMatrix(myCorpus, control = list(minWordLength = 1))
myDtm <- TermDocumentMatrix(myCorpus, control = list(minWordLength = 1))
myCorpus <- Corpus(VectorSource(myCorpus))
myCorpus <- tm_map(myCorpus, tolower)
myCorpus <- tm_map(myCorpus, PlainTextDocument)
myCorpus <- tm_map(myCorpus, removePunctuation)
myCorpus <- tm_map(myCorpus, removeNumbers)
myCorpus <- tm_map(myCorpus, PlainTextDocument)
myStopwords <- c(stopwords('english'), "available", "via")
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
dictCorpus <- myCorpus
myCorpus <- tm_map(myCorpus, stemDocument)
myCorpus <- tm_map(myCorpus, stemCompletion, dictionary=dictCorpus)
myDtm <- TermDocumentMatrix(myCorpus, control = list(minWordLength = 1))
myCorpus <- Corpus(VectorSource(tweets.df$text))
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
myCorpus <- tm_map(myCorpus, removePunctuation)
myCorpus <- tm_map(myCorpus, removeNumbers)
myStopwords <- c(stopwords('english'), "available", "via")
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
myCorpus <- tm_map(myCorpus, stemDocument)
myCorpus <- tm_map(myCorpus, stemCompletion, dictionary=dictCorpus)
myDtm <- TermDocumentMatrix(myCorpus, control = list(minWordLength = 1))
myCorpus <- tm_map(myCorpus, PlainTextDocument)
myDtm <- TermDocumentMatrix(myCorpus, control = list(minWordLength = 1))
inspect(myDtm[1:3,31:40])
myDtm
inspect(myDtm[266:270,31:40])
findFreqTerms(myDtm, lowfreq=10)
findAssocs(myDtm, 'r', 0.30)
q()
?choose
?outer
setwd("~/Kaggle/Churn")
churn <- read.csv("GramenerTraining.csv", header=T)
churn2 <- read.csv("churn2.csv", header=T)
keepvars1 <- c( "Churn",
"Int.l.Plan",
"Day.Mins",
"CustServ.Calls",
"Message.Plan",
"Eve.Mins",
"Intl.Charge",
"Intl.Calls",
"State.CA",
"State.KS",
"State.MA",
"State.MI",
"State.NJ",
"State.NV",
"State.NY",
"State.SC",
"State.TX")
churn3 <- churn2[, keepvars1]
train = churn3[1:nrow(churn3)*0.8,]
label = train$Churn
train$Churn <- NULL
m2 = gbm.step(data = churn3, gbm.x = 2:17, gbm.y = 1, family = "bernoulli",
learning.rate = 0.01, tree.complexity = 5, bag.fraction = 0.7)
library(dismo)
m2 = gbm.step(data = churn3, gbm.x = 2:17, gbm.y = 1, family = "bernoulli",
learning.rate = 0.01, tree.complexity = 5, bag.fraction = 0.7)
par(mfcol=c(4,4))
gbm.plot(m2, n.plots=16, write.title=F)
gbm.plot.fits(m2)
?gbm.plot.fits
gbm.plot.fits(m2, v=c(2:10))
gbm.plot.fits(m2, v=c(11:16))
find.int <- gbm.interactions(m2)
find.int$interactions
find.int$rank.list
gbm.perspec(m2, 13, 1, y.range = c(20, 2770), z.range = c(0, 0.6))
dev.off()
gbm.perspec(m2, 13, 1, y.range = c(20, 2770), z.range = c(0, 0.6))
gbm.perspec(m2, 2, 4, y.range = c(20, 2770), z.range = c(0, 0.6))
preds <- predict.gbm(m2, churn3, n.trees=m2$gbm.call$best.trees, type="response")
library(gbm)
preds <- predict.gbm(m2, churn3, n.trees=m2$gbm.call$best.trees, type="response")
calc.deviance((obs=churn3$Churn, pred=preds, calc.mean=T))
calc.deviance(obs=churn3$Churn, pred=preds, calc.mean=T)
d <- cbind(churn3$Churn, preds)
pres <- d[d[,1]==1, 2]
abs <- d[d[,1]==0, 2]
e <- evaluate(p=pres, a=abs)
e
?evaluate
churn3$preds <- predict.gbm(m2, churn3, n.trees=m2$gbm.call$best.trees, type="response")
head(churn3)
library(pROC)
auc <- roc(churn3$Churn, churn3$preds)
auc$auc
plot(auc)
churn_sor <- churn3[order(-churn3$preds),]
plot(churn3$preds, xlab="Ranked Customers", ylab="Predicted Probabilities",
main="Distribution of Probabilities of Churn")
plot(churn_sor$preds, xlab="Ranked Customers", ylab="Predicted Probabilities",
main="Distribution of Probabilities of Churn")
churn_sor$nonchurn <- 1 - churn_sor$Churn
decile <- function(x){
deciles <- vector(length=10)
for (i in seq(0.1,1,.1)){
deciles[i*10] <- quantile(x, i)
}
return (ifelse(x<deciles[1], 1,
ifelse(x<deciles[2], 2,
ifelse(x<deciles[3], 3,
ifelse(x<deciles[4], 4,
ifelse(x<deciles[5], 5,
ifelse(x<deciles[6], 6,
ifelse(x<deciles[7], 7,
ifelse(x<deciles[8], 8,
ifelse(x<deciles[9], 9, 10))))))))))
}
churn_sor$scrdec <- 11 - decile(churn_sor$preds)
groupd <- ddply(churn_sor, c("scrdec"), summarise,
N = length(Churn),
sumesc = sum(Churn),
sumnesc = sum(nonchurnhurn),
avg_esc_scr = mean(preds)
)
groupd
library(plyr)
groupd <- ddply(churn_sor, c("scrdec"), summarise,
N = length(Churn),
sumesc = sum(Churn),
sumnesc = sum(nonchurnhurn),
avg_esc_scr = mean(preds)
)
groupd
groupd <- ddply(churn_sor, c("scrdec"), summarise,
N = length(Churn),
sumesc = sum(Churn),
sumnesc = sum(nonchurn),
avg_esc_scr = mean(preds)
)
groupd
groupd$cum_esc <- as.numeric(formatC(cumsum(groupd$sumesc)))
groupd$cum_nesc <- as.numeric(formatC(cumsum(groupd$sumnesc), format="f"))
groupd$cum_n <- as.numeric(formatC(cumsum(groupd$N)))
tot_esc <- as.numeric(groupd$cum_esc[10])
tot_nesc <- as.numeric(groupd$cum_nesc[10])
groupd$cum_esc_pct <- groupd$cum_esc / tot_esc
groupd$cum_nesc_pct <- groupd$cum_nesc / tot_nesc
groupd$KS <- abs(groupd$cum_esc_pct - groupd$cum_nesc_pct) * 100
groupd
max(groupd$KS)
coef(m2)
coefficients(m2)
summary(m2)
plot(groupd$scrdec, groupd$cum_esc_pct, xlab="Deciles", ylab="Ratio of Population Captured",
yaxt="n", main = "Predicted Churn Lift Chart", type ="l", col="red", lwd=2, xlim=c(1,10),ylim=c(0,1))
axis(side=2, at=c(seq(0,1,0.1)))
lines(groupd$cum_nesc_pct, col="green", lwd=2)
?evaluate
test <- read.csv("GramenerTesting.csv", header=T)
keepvars <- c(  "Int.l.Plan",
"Day.Mins",
"CustServ.Calls",
"Message.Plan",
"Eve.Mins",
"Intl.Charge",
"Intl.Calls",
)
test1 <- test[, keepvars]
keepvars <- c(  "Int.l.Plan",
"Day.Mins",
"CustServ.Calls",
"Message.Plan",
"Eve.Mins",
"Intl.Charge",
"Intl.Calls"
)
test1 <- test[, keepvars]
head(test)
keepvars <- c(  "Int.l.Plan",
"Day.Mins",
"CustServ.Calls",
"Message.Plan",
"Evening.Mins",
"Intl.Charge",
"Intl.Calls"
)
test1 <- test[, keepvars]
test1$Eve.Mins <- test1$Evening.Mins
test1$Evening.Mins <- NULL
state <- predict(dummyVars(~ State, data = test), newdata = test)
library(caret)
state <- predict(dummyVars(~ State, data = test), newdata = test)
test2 <- as.data.frame(cbind(test1, state))
test2$State <- NULL
keepvars1 <- c( "Churn",
"Int.l.Plan",
"Day.Mins",
"CustServ.Calls",
"Message.Plan",
"Eve.Mins",
"Intl.Charge",
"Intl.Calls",
"State.CA",
"State.KS",
"State.MA",
"State.MI",
"State.NJ",
"State.NV",
"State.NY",
"State.SC",
"State.TX")
test3 <- test2[, keepvars1]
str(test1)
keepvars1 <- c(
"Int.l.Plan",
"Day.Mins",
"CustServ.Calls",
"Message.Plan",
"Eve.Mins",
"Intl.Charge",
"Intl.Calls",
"State.CA",
"State.KS",
"State.MA",
"State.MI",
"State.NJ",
"State.NV",
"State.NY",
"State.SC",
"State.TX")
test3 <- test2[, keepvars1]
test3$preds <- predict.gbm(m2, test3, n.trees=m2$gbm.call$best.trees, type="class")
test3$preds <- predict.gbm(m2, test3, n.trees=m2$gbm.call$best.trees, type="response")
?predict.gbm
test3$preds <- predict.gbm(m2, test3, n.trees=m2$gbm.call$best.trees, type="response")
p.preds <- apply(test3$preds, 1, which.max)
test3$preds <- predict.gbm(m2, newdata = test3, n.trees=m2$gbm.call$best.trees, type="response")
head(test3)
p.preds <- apply(test3$preds, 1, which.max)
dim(test3$preds)
preds <- predict.gbm(m2, newdata = test3, n.trees=m2$gbm.call$best.trees, type="response")
p.preds <- apply(preds, 1, which.max)
head(preds)
preds[1:6,,]
head(test3)
test3$preds <- predict.gbm(m2, newdata = test3, n.trees=m2$gbm.call$best.trees, type="link")
head(test3)
test3$preds <- predict.gbm(m2, newdata = test3, n.trees=m2$gbm.call$best.trees, type="response")
head(test3)
?predict.gbm
confusionMatrix(churn3$Churn, churn3$preds)
test3 <- test3[order(test3$preds), ]
test3 <- test3[order(test3$preds), ]
plot(test3$preds)
quantile(test3$preds, probs=seq(0,1,0.1))
pred_class <- apply(pred, 1 which.max)
pred_class <- apply(pred, 1, which.max)
pred_class <- apply(preds, 1, which.max)
str(preds)
length(preds)
dim(preds)
preds <- predict.gbm(m2, newdata = test3, n.trees=m2$gbm.call$best.trees, type="response")
dim(preds) <- c(666,1)
pred_class <- apply(preds, 1, which.max)
head(pred_class)
test4 <- as.data.frame(cbind(test3, pred_class))
head(test4)
prop.table(table(test4$pred_class))
test3$pred_class <- ifelse(test3$preds > 0.5, 1, 0)
preds <- predict.gbm(m2, newdata = test3, n.trees=m2$gbm.call$best.trees, type="response")
test3 <- as.data.frame(cbind(test, preds))
test3$pred_class <- ifelse(test$preds > 0.5, 1, 0)
test3$pred_class <- ifelse(test3$preds > 0.5, 1, 0)
str(test3)
submit <- test3[, c("Area.Code", "Phone", "pred_class")]
head(submit)
prop.table(table(test3$pred_class))
write.csv(submit, file="submit.csv")
q()
